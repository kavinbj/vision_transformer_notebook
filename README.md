<!--
 * @Author: kavinbj
 * @Date: 2022-08-23 18:37:28
 * @LastEditTime: 2022-09-08 17:10:34
 * @FilePath: README.md
 * @Description: 
 * 
 * Copyright (c) 2022 by kavinbj, All Rights Reserved. 
-->

# CV transformer 相关 学习笔记


### Transformer 

### Bert

### ViT
![ViT structure](https://pic1.zhimg.com/80/v2-5ac1b11cdab826232652def8e44a4828_1440w.jpg)


### MAE

### SegFormer

### DETR

![DETR structure](https://pic3.zhimg.com/80/v2-46321049b6ca8f86ca7fd7eb80443b96_1440w.jpg)


### DeiT

### Efficient Transformer 

### Mobile-Transformer

### Swin Transformer
![Swin Transformer structure](https://pic3.zhimg.com/80/v2-4ed9fd14ccf3720a670150400e76432e_1440w.jpg)




## attention 相关
* [attention and self attention](https://github.com/kavinbj/vision_transformer_notebook/blob/main/notes/attention.md) 注意力机制与自注意力机制的理解




RNN --> LSTM --> GRU

* [RNN](https://motor.readthedocs.io/en/stable/index.html)


# Autoregressive


![RUNOOB 图标](https://github.com/kavinbj/vision_transformer_notebook/blob/main/imgs/attention01.jpg)


<!-- ![RUNOOB 图标](https://pic4.zhimg.com/80/v2-3a88ac6c530170672781ae63ec695c83_1440w.jpg) -->

# multi-modality problem

# autoregressive decoder VS non-autoregressive decoder

![RUNOOB 图标](https://pic3.zhimg.com/80/v2-479670505986aa912ccaf17e1238c446_1440w.jpg)


batch normal vs layer normal




