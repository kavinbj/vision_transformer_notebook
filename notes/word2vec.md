
# word2vec 学习笔记

### 1、word2vec简介

Word2vec是一种词嵌入技术，用来进行文本表示的。首先谈一下文本表示，众所周知，机器是无法识别和理解自然语言的。文本表示就是把自然语言转换成数字或向量，进行建模，让机器能够认识且更好的理解文本信息。其实，NLP的终极目标就是**让计算机能够确切理解人类的语言，并自然地与人进行交互。**


**在word2vec技术诞生之前，文本表示的方法主要有独热编码（one-hot），TF-IDF等等**，独热编码实现细节是，首先根据文本内容构建一个固定顺序的词表，在出现词对应的词表位置用1表示，没出现的词用0表示。这种方法的优点是原理简单，使用方便。缺点是没有考虑语义相关信息，且容易导致维度“灾难”等。


![word2vec one-hot](https://pic3.zhimg.com/80/v2-6da199f86e73a271debd4bb20d62f316_1440w.jpg)
*one-hot表示形式*


2013年，Mikolov等人针对神经网络语言模型（NNLM）的弊端，提出了词向量文本表示方法word2vec。该方法仅采用只有一个隐藏层的神经网络，把one-hot形式的高维、稀疏的向量输入，映射成一个低维、稠密词向量的文本表示方法。


### 2、Word2vec模型结构及实现原理

word2vec模型其实就是简单化的神经网络。

![word2vec模型结构](https://pic4.zhimg.com/80/v2-b8caa21289bfa86fcad15bb5a137cb13_1440w.jpg)
*word2vec模型结构*

word2vec包含CBOW和Skip-gram两种词向量训练模型，如图2所示。CBOW连续词袋模型，即通过上下文的K个单词来预测中心词w（t），而Skip-gram模型是通过中心词w（t）来预测上下文K个单词。为了提高词向量的训练速度常采用负采样（Negative Sample）和Hierarchical Softmax技术。

![word2vec model](https://pic3.zhimg.com/80/v2-a74d9296c2785d62bdad6c77bd2e0fc6_1440w.jpg)
*word2vec模型*

对每个训练过程做如此庞大的计算是非常昂贵的，使得它难以扩展到词汇表或者训练样本很大的任务中去。为了解决这个问题，我们直观的想法就是限制每次必须更新的输出向量的数量。一种有效的手段就是采用分层softmax；另一种可行的方法是通过负采样。


### 3、Word2vec的优缺点
word2vec在大规模的语料库上进行训练，通过对训练的结果进行分析，发现几个有趣的现象，这几个表现也在一定程度上反映了word2vec的优点。

![word2vec model](https://pic1.zhimg.com/80/v2-60c69c5818359743c775b6b00926d148_1440w.jpg)
*相似实体词的词向量在空间中的分布呈聚集性*


![word2vec model](https://pic3.zhimg.com/80/v2-a25c04284dfac59047d6380f82addcfe_1440w.jpg)
*不同语言，同一词在空间中的位置，几乎是重合的*

![word2vec model](https://pic3.zhimg.com/80/v2-4bf7b6bb5dc3dfe019ac94988347ff1e_1440w.jpg)
*词向量国王到王后的距离=男人到女人的距离，这也反映了word2vec在一定程度上是可以学习到词的含义的。*

优点：word2vec能够较好的考虑上下文语义信息，同时可以避免维度“灾难”问题。

缺点：word2vec的缺点是，它是一种静态的词向量表示方式，词与向量一对一固定表示，这会导致多义词无法正确表示。例如‘’苹果‘’即可代表水果，又可以代表手机品牌。



